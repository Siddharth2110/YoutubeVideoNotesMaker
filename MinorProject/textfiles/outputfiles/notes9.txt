 What is a web crawler botA web crawler, spider, or search engine bot downloads and indexes content from all over the Internet. The goal ofsuch a bot is to learn what almost every webpage on the web is about, so that the information can be retrievedwhen its needed. Theyre called web crawlers because crawling is the technical term for automatically accessinga website and obtaining data via a software program.These bots are almost always operated by search engines. By applying a search algorithm to the data o the data o the data collected by webcrawlers, search engines can provide relevant links in response to user search queries, generating the list of webpages thatshow up after a user types a search into Google or Bing or another search engine.A web crawler bot is like someone who goes through all the books in a disorganized library and puts together a card catalogso that anyone who visits the library can quickly and easily find the information they need. To help categorize and sort thelibrarys books by topic, the organizer will read the title, summary, and some of the internal text of each book to figure outwhat its about. ts about. unlike a library, the Internet is not composed of physical piles of books, and that makes it hard to tell ifall the necessary information has been indexed properly, or if vast quantities of it are being overlooked. To try tofind all the relevant information the Internet has to offer, a web crawler bot will start with a certain set of knownwebpages and then follow hyperlinks from those pages to other pages, follow hyperlinks from those other pagesto additional pages, and so on.It is unknown how much of the publicly available Internet is actually crawled by search engine bots. Some sourcesestimate that only 40-70 of the Internet is indexed for search  and thats billions of webpages. webpages. webpages. earch indexingSearch indexing is like creating a library card catalog for the Internet so that a searchengine knows where on the Internet to retrieve information when a person searches forit. It can also be compared to the index in the back of a book, which lists all the placesin the book where a certain topic or phrase is mentioned.Indexing focuses mostly on the text that appears on the page, and on the metadataabout the page that users dont see. When most search engines index a page, they addall the words on the page to the index  except for words like a, an, and the inGoogles case. When users search for those words, the search engine goes through itsindex of all the pages where those words appear and selects the most relevant ones. nes. nes. ive importance of each webpage Most web crawlers dont crawl the entire publicly available Internet and arentintended to instead they decide which pages to crawl first based on the number of other pages that link to that page, theamount of visitors that page gets, and other factors that signify the pages likelihood of containing important information.The idea is that a webpage thatis cited by a lot of other webpages and gets a lot of visitors is likely to containhigh-quality, authoritative information, so its especially important that a search engine has it indexed  just as a librarymight make sure to keep plenty of copies of a book that gets checked out by lots of people.Revisiting webpages Content on the Web is continually being updated, removed, or moved to new locations. Web crawlerswill periodically need to revisit pages to make sure the latest version of the content is indexed. exed. exed. t requirements Web crawlers also decide which pages to crawl based on the robots.txt protocol also known asthe robots exclusion protocol. Before crawling a webpage, they will check the robots.txt file hosted by that pages webserver. A robots.txt file is a text file that specifies the rules for any bots accessing the hosted website or application. Theserules define which pages the bots can crawl, and which links they can follow. As an example, check out the Cloudflare.comrobots.txt file. . . . . 